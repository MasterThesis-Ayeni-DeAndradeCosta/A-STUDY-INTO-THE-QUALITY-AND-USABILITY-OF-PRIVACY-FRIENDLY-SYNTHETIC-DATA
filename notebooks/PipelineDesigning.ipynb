{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipeline**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "necessary imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install sdv\n",
    "import pandas as pd\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import CTGANSynthesizer\n",
    "import os\n",
    "from sdv.evaluation.single_table import run_diagnostic, evaluate_quality, get_column_plot\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What gets input into the pipeline?\n",
    "\n",
    "- The Dataset\n",
    "- The variable which we will be predicting\n",
    "- *(Maybe which ML models are relevant to the task?)*\n",
    "- *(Maybe the separator for the csv file, or we could write in the documentation that only csv files with \";\" as a separator is accepted)*\n",
    "\n",
    "## Preprocessing will be done before putting the dataset into the pipeline\n",
    "\n",
    "- Consistent CSV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Synthesization Part**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input data to the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dataset_path = \"../datasets/original/studentPerformance.csv\"\n",
    "separator = ';'\n",
    "target_column = 'Target'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating the Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract dataset name from path (gets 'studentPerformance' from the path)\n",
    "dataset_name = os.path.splitext(os.path.basename(original_dataset_path))[0]\n",
    "\n",
    "original_data =pd.read_csv(original_dataset_path, sep=separator) #make it into a Pandas dataFrame (easier to work with)\n",
    "\n",
    "print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "print(f\"Original dataset size: {len(original_data)} rows\")\n",
    "\n",
    "metadata = SingleTableMetadata() # Initialize metadata\n",
    "\n",
    "metadata.detect_from_dataframe(original_data) # Detect metadata from the pandas dataFrame\n",
    "\n",
    "\n",
    "synthesizer_path = f\"../src/synthesizers/{dataset_name}_synthesizer.pkl\"\n",
    "\n",
    "# Check if already existing synthesizer exists , if not create one\n",
    "if os.path.exists(synthesizer_path):\n",
    "    print(f\"\\nFound existing synthesizer for {dataset_name} at: {synthesizer_path}\")\n",
    "    print(\"Loading synthesizer...\")\n",
    "    synthesizer = CTGANSynthesizer.load(synthesizer_path)\n",
    "    print(\"Synthesizer loaded successfully\")\n",
    "else:\n",
    "    print(f\"\\nNo existing synthesizer for {dataset_name} found.\")\n",
    "    print(\"Training new synthesizer... (this may take a while)\")\n",
    "    synthesizer = CTGANSynthesizer(metadata)\n",
    "    synthesizer.fit(original_data)\n",
    "    synthesizer.save(synthesizer_path)\n",
    "    print(\"New synthesizer trained and saved successfully\")\n",
    "\n",
    "# Generate synthetic data\n",
    "print(f\"\\nUsing synthesizer '{dataset_name}_synthesizer' to generate synthetic data for dataset '{dataset_name}'\")\n",
    "synthetic_data = synthesizer.sample(num_rows=len(original_data))\n",
    "print(f\"Synthetic data generated successfully: {len(synthetic_data)} rows created\")\n",
    "synthetic_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sdv.evaluation.single_table import run_diagnostic\n",
    "\n",
    "# Run diagnostic\n",
    "print(f\"\\nRunning diagnostic comparison for {dataset_name}...\")\n",
    "diagnostic = run_diagnostic(\n",
    "    real_data=original_data,\n",
    "    synthetic_data=synthetic_data,\n",
    "    metadata=metadata\n",
    ")\n",
    "\n",
    "# Quality evaluation\n",
    "print(f\"\\nEvaluating quality metrics for {dataset_name}...\")\n",
    "quality_report = evaluate_quality(\n",
    "    original_data,\n",
    "    synthetic_data,\n",
    "    metadata\n",
    ")\n",
    "\n",
    "# Get details of column shapes\n",
    "print(\"\\nAnalyzing column distributions...\")\n",
    "column_shapes = quality_report.get_details('Column Shapes')\n",
    "print(column_shapes)\n",
    "\n",
    "quality_report.get_details('Column Shapes')\n",
    "\n",
    "print(\"\\nGenerating sample column distribution plot...\")\n",
    "fig = get_column_plot(\n",
    "    real_data=original_data,\n",
    "    synthetic_data=synthetic_data,\n",
    "    column_name= target_column,  \n",
    "    metadata=metadata\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Machine Learning Part**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#original data is split into training and test\n",
    "def prepare_original_data(data, target_column):\n",
    "   X = data.drop(target_column, axis=1)\n",
    "   y = data[target_column]\n",
    "   return train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Prepare synthetic data (no splitting needed)\n",
    "def prepare_synthetic_data(data, target_column):\n",
    "   X = data.drop(target_column, axis=1)\n",
    "   y = data[target_column] \n",
    "   return X, y\n",
    "\n",
    "# Prepare datasets using the target_column variable\n",
    "X_train_original, X_test_original, y_train_original, y_test_original = prepare_original_data(original_data, target_column)\n",
    "X_synthetic, y_synthetic = prepare_synthetic_data(synthetic_data, target_column)\n",
    "\n",
    "\n",
    "datasets = {\n",
    "   'Original': (X_train_original, y_train_original), # Training data only\n",
    "   'Synthetic': (X_synthetic, y_synthetic) # Full synthetic data\n",
    "}\n",
    "\n",
    "model_results = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the models we will be evaluating\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=2000),\n",
    "    'KNN': KNeighborsClassifier(),\n",
    "    'Random Forest': RandomForestClassifier(),\n",
    "    'Decision Tree': DecisionTreeClassifier(),\n",
    "    'SVM': SVC()\n",
    "}\n",
    "\n",
    "\n",
    "# Iterate through each dataset\n",
    "for dataset_name, (X_train, y_train) in datasets.items():\n",
    "    print(f'\\nTraining and evaluating models on {dataset_name} dataset:')\n",
    "    \n",
    "    # Iterate through each model\n",
    "    for model_name, model in models.items():\n",
    "        print(f'\\nProcessing {model_name}...')\n",
    "        \n",
    "        # Train and evaluate\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Cross validation score\n",
    "        scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy')\n",
    "        \n",
    "        # Predict on original test set\n",
    "        predictions = model.predict(X_test_original)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "            y_test_original, \n",
    "            predictions, \n",
    "            average='weighted'\n",
    "        )\n",
    "        \n",
    "        # Store results\n",
    "        model_results.append({\n",
    "            'Dataset': dataset_name,\n",
    "            'Model': model_name,\n",
    "            'Accuracy': round(np.mean(scores), 4),\n",
    "            'Precision': round(precision, 4),\n",
    "            'Recall': round(recall, 4),\n",
    "            'F1': round(f1, 4)\n",
    "        })\n",
    "        \n",
    "        print(f'Completed {model_name} evaluation')\n",
    "\n",
    "results_df = pd.DataFrame(model_results)\n",
    "\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = []\n",
    "original_accuracy_scores = []\n",
    "original_precision_scores = []\n",
    "synthetic_accuracy_scores = []\n",
    "synthetic_precision_scores = []\n",
    "\n",
    "# Separate original and synthetic results\n",
    "original_results = results_df[results_df['Dataset'] == 'Original']\n",
    "synthetic_results = results_df[results_df['Dataset'] == 'Synthetic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in original_results['Model'].unique():\n",
    "    model_names.append(model)\n",
    "    original_accuracy_scores.append(original_results[original_results['Model'] == model]['Accuracy'].values[0])\n",
    "    original_precision_scores.append(original_results[original_results['Model'] == model]['Precision'].values[0])\n",
    "    synthetic_accuracy_scores.append(synthetic_results[synthetic_results['Model'] == model]['Accuracy'].values[0])\n",
    "    synthetic_precision_scores.append(synthetic_results[synthetic_results['Model'] == model]['Precision'].values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create visualization\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(16, 6))\n",
    "\n",
    "x = np.arange(len(model_names))\n",
    "width = 0.35\n",
    "\n",
    "# Accuracy plot\n",
    "ax1.bar(x - width/2, original_accuracy_scores, width, label='Original Data', color='skyblue')\n",
    "ax1.bar(x + width/2, synthetic_accuracy_scores, width, label='Synthetic Data', color='lightcoral')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Accuracy Comparison')\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels(model_names, rotation=45)\n",
    "ax1.legend()\n",
    "\n",
    "# Precision plot\n",
    "ax2.bar(x - width/2, original_precision_scores, width, label='Original Data', color='skyblue')\n",
    "ax2.bar(x + width/2, synthetic_precision_scores, width, label='Synthetic Data', color='lightcoral')\n",
    "ax2.set_ylabel('Precision')\n",
    "ax2.set_title('Precision Comparison')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(model_names, rotation=45)\n",
    "ax2.legend()\n",
    "\n",
    "plt.suptitle(f'Model Performance: Original vs Synthetic Data for {dataset_name}', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDetailed Model Performance Comparison:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
